+++
title = "ã¯ã˜ã‚ã¦ã® Deep Learning - Keras ã§ MLP for MNIST"
tags = [ "keras", "tensorflow", "python" ]
date = "2016-07-15T04:38:39+09:00"
+++

å‹•æ©Ÿã¯ã•ã¦ãŠãã€[ã“ã¡ã‚‰ã®ã‚¨ãƒ³ãƒˆãƒª](http://aidiary.hatenablog.com/entry/20160328/1459174455) ã‚’èª­ã‚“ã§æ°—ã«ãªã£ã¦ã„ãŸ Keras ã‚’è§¦ã£ã¦ã¿ãŸã®ã§ãƒ¡ãƒ¢ã€‚è‡ªåˆ†ã¯æ©Ÿæ¢°å­¦ç¿’ã«ã‚‚ Python ã«ã‚‚è§¦ã‚ŒãŸã“ã¨ã¯ãªã„ã®ã§ã€ã¨ã‚Šã‚ãˆãšã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è§£ããªãŒã‚‰ã€èª°ã—ã‚‚ãŒé€šã‚‹ã§ã‚ã‚ã†ï¼ˆï¼Ÿï¼‰MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è­˜å­—ã‚’ã‚„ã£ã¦ã¿ãŸã€‚è¡¨é¡Œã®é€šã‚Šã€ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã¯ MLPï¼ˆMulti-Layer Perceptronï¼‰ã€‚ã¾ãŸã€ä»Šå›æã„ãŸã‚³ãƒ¼ãƒ‰ã«ã¯ä¸å¯§ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã¤ã‘ãŸã¤ã‚‚ã‚Šãªã®ã§ã€åŒã˜ã“ã¨ã‚’ã‚„ã‚ã†ã¨ã—ã¦ã„ã‚‹æ–¹ã®æ‰‹åŠ©ã‘ã«ãªã‚Œã°å¹¸ã„ã§ã™ğŸ™

<!--more-->

## Keras

http://keras.io/ja

> Keras ã¯æœ€å°é™ã§è¨˜è¿°ã§ãã‚‹,ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã«å¯¾å¿œã—ã¦ã„ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚Pythonã«ã‚ˆã£ã¦è¨˜è¿°ã•ã‚Œã¦ãŠã‚Šã€Tensorflowã‚„Theanoã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ é©æ–°çš„ãªç ”ç©¶ã€é–‹ç™ºã‚’è¡Œã†ãŸã‚ã«ã¯ã‚¢ã‚¤ãƒ‡ã‚¢ã‹ã‚‰çµæœã¾ã§æœ€å°é™ã®æ™‚é–“ã§è¡Œã†ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ãã“ã§Kerasã¯ã‚ˆã‚Šæ—©ã„å®Ÿè£…ã€æ”¹è‰¯ã‚’è¡Œã†ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦é–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚

æ—¥æœ¬èªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã‚ã£ã¦ã€ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚‚å…±æ„Ÿã§ãã‚‹ã€‚Google å…ˆç”Ÿã® [TensorFlow](https://www.tensorflow.org)ï¼ˆã“ã¡ã‚‰ã‚‚è§¦ã£ã¦ã¿ãŸã‹ã£ãŸï¼‰ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã„ã†ã‚«ã‚¿ãƒã§ãƒ©ãƒƒãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹ã‚‰ã—ã„ã€‚ãªãŠã€Keras è‡ªä½“ã®ä½œè€…ã‚‚ Google ã®æ–¹ã®æ¨¡æ§˜ã€‚

## æº–å‚™

``` sh
$ python --version
Python 3.5.1
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯ä»¥ä¸‹ã§çµ‚ã‚ã‚Šã€‚ç°¡å˜ã€‚

``` sh
$ pip install keras
$ pip list | grep Keras
Keras (1.0.5)
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’æ„šç›´ã«å®Ÿè¡Œ

GitHub ã«ã‚ã‚‹ [ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py) ã‚’è½ã¨ã—ã¦ãã¦å®Ÿè¡Œã—ã¦ã¿ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚‹ã€‚å­¦ç¿’ã—ã¦ã„ã‚‹é›°å›²æ°—ãŒå‡ºã¦ã„ã¦ã€çœºã‚ã¦ã„ã‚‹ã ã‘ã§æ¥½ã—ã„ã€‚

``` sh
$ python mnist_mlp.py
Using Theano backend.
60000 train samples
10000 test samples
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
dense_1 (Dense)                  (None, 512)           401920      dense_input_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 512)           0           dense_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 512)           0           activation_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 512)           262656      dropout_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 512)           0           dense_2[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 512)           0           activation_2[0][0]
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 10)            5130        dropout_2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 10)            0           dense_3[0][0]
====================================================================================================
Total params: 669706
____________________________________________________________________________________________________
Train on 60000 samples, validate on 10000 samples
Epoch 1/20
 9600/60000 [===>..........................] - ETA: 9s - loss: 0.5566 - acc: 0.8247
```

ã§ã€æœ€çµ‚çš„ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªçµæœã¨ãªã£ãŸã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ 98.16% ã§è­˜å­—ã§ãã¦ã„ã‚‹ã‚‰ã—ã„ã€‚ã™ã”ã„ã€‚

``` sh
Epoch 19/20
60000/60000 [==============================] - 13s - loss: 0.0192 - acc: 0.9955 - val_loss: 0.1210 - val_acc: 0.9820
Epoch 20/20
60000/60000 [==============================] - 13s - loss: 0.0189 - acc: 0.9953 - val_loss: 0.1194 - val_acc: 0.9816
Test score: 0.11935520198
Test accuracy: 0.9816
```

## ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ TensorFlow ã«åˆ‡ã‚Šæ›¿ãˆã‚‹

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯ [Theano](http://deeplearning.net/software/theano) ãªã®ã§ã€ã“ã‚Œã‚’ TensorFlow ã«åˆ‡ã‚Šæ›¿ãˆã¦ã¿ã‚‹ã€‚ã¨ã„ã£ã¦ã‚‚ã€ã‚„ã‚‹ã“ã¨ã¯ `~/.keras/keras.json`ï¼ˆä¸Šè¨˜ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ãŸéš›ã«ã§ãã¦ã„ã‚‹ã¯ãšï¼‰ã®ä¸­ã® `backend` ã‚’ `theano` ã‹ã‚‰ `tensorflow` ã«æ›¸ãæ›ãˆã‚‹ã ã‘ã€‚

TensorFlow è‡ªä½“ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ [ã“ã¡ã‚‰](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md)ã€‚pip ã‚’ä½¿ãˆã°ç°¡å˜ã«ã§ãã‚‹ã€‚ã¡ãªã¿ã«ã€è‡ªåˆ†ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ä»¥ä¸‹ã€‚

``` sh
$ pip list | grep tensorflow
tensorflow (0.9.0)
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è§£ãã¤ã¤ã€æ•´ç†ã—ã¦ã¿ã‚‹

ã¨ã‚Šã‚ãˆãšã€åˆã£ç«¯ã‹ã‚‰ã‚ã‹ã‚‰ãªã„ã€‚ã¾ãšã¯ä»¥ä¸‹ã®éƒ¨åˆ†ã€‚

``` python
# the data, shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

ãªã«ã‚„ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚è€ƒãˆã¦ã‚‚ã‚ˆãã‚ã‹ã‚“ãªã„ã®ã§ä¸­èº«ã‚’å‡ºåŠ›ã—ã¦ã¿ã‚‹ã“ã¨ã«ã—ãŸã€‚Python ã‚‚åˆã‚ã¦ã ã‘ã©ã€ã‚°ã‚°ã‚Šã¤ã¤ãŒã‚“ã°ã‚‹ã€‚ã¨ã‚Šã‚ãˆãšã€`X_train` ã¨ `y_train` ã«çµã£ã¦ä¸­èº«ã‚’è¦‹ã¦ã¿ã‚‹ã€‚

``` python
# -*- coding: utf-8 -*-

import sys
import numpy as np
np.random.seed(20160715)

from keras.datasets import mnist
from keras.utils import np_utils

(X_train, y_train), (X_test, y_test) = mnist.load_data()

for xs in X_train[0]:
    for x in xs:
        sys.stdout.write('%03d ' % x)
    sys.stdout.write('\n')

print('first sample is %d' % y_train[0])

Y_train = np_utils.to_categorical(y_train, 10)

sys.stdout.write('[')
for y in Y_train[0]:
    sys.stdout.write('%f ' % y)
sys.stdout.write(']\n'))
```

ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€å‡ºåŠ›ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚

``` txt
Using TensorFlow backend.
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 003 018 018 018 126 136 175 026 166 255 247 127 000 000 000 000
000 000 000 000 000 000 000 000 030 036 094 154 170 253 253 253 253 253 225 172 253 242 195 064 000 000 000 000
000 000 000 000 000 000 000 049 238 253 253 253 253 253 253 253 253 251 093 082 082 056 039 000 000 000 000 000
000 000 000 000 000 000 000 018 219 253 253 253 253 253 198 182 247 241 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 080 156 107 253 253 205 011 000 043 154 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 014 001 154 253 090 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 139 253 190 002 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 011 190 253 070 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 035 241 225 160 108 001 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 081 240 253 253 119 025 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 045 186 253 253 150 027 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 016 093 252 253 187 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 249 253 249 064 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 046 130 183 253 253 207 002 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 039 148 229 253 253 253 250 182 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 024 114 221 253 253 253 253 201 078 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 023 066 213 253 253 253 253 198 081 002 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 018 171 219 253 253 253 253 195 080 009 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 055 172 226 253 253 253 253 244 133 011 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 136 253 253 253 212 135 132 016 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000
first sample is 5
[0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 ]
```

5 ã£ã½ã„ï¼ï¼ï¼5 ã£ã½ã„ã‚ˆï¼ï¼ï¼ï¼ï¼

ã¨ã„ã†ã“ã¨ã§ã€ã“ã‚Œä»¥å¤–ã«ã‚‚ã„ã‚ã„ã‚ä¸­èº«ã‚’å‡ºåŠ›ã—ã¦ã¿ã¦ã‚ã‹ã£ãŸã“ã¨ã‚’æ•´ç†ã™ã‚‹ã¨ã€ã©ã†ã‚„ã‚‰ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã‚‰ã—ã„ã€‚

- `X_train`ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åŠ›ï¼‰
- `X_test`ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå…¥åŠ›ï¼‰
- `Y_train`ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆå‡ºåŠ›ï¼‰
- `Y_test`ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå‡ºåŠ›ï¼‰

``` python
# MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# å¤‰æ›å‰ï¼š28 x 28 ã®2æ¬¡å…ƒé…åˆ— x 60,000
# å¤‰æ›å¾Œï¼š784è¦ç´ ã®1æ¬¡å…ƒé…åˆ— x 60,000ï¼ˆ256éšèª¿ã‚’ 0 ã€œ 1 ã«æ­£è¦åŒ–ï¼‰
X_train = X_train.reshape(60000, 784).astype('float32') / 255
X_test  = X_test.reshape(10000, 784).astype('float32') / 255

# å¤‰æ›å‰ï¼š0 ã€œ 9 ã®æ•°å­— x 60,000
# å¤‰æ›å¾Œï¼š10è¦ç´ ã®1æ¬¡å…ƒé…åˆ—ï¼ˆone-hot è¡¨ç¾ï¼‰ x 60,000
#         - 0 : [1,0,0,0,0,0,0,0,0,0]
#         - 1 : [0,1,0,0,0,0,0,0,0,0]
#         ...
Y_train = np_utils.to_categorical(y_train, 10)
Y_test  = np_utils.to_categorical(y_test, 10)
```

ãµã‚€ã€‚ãªã‚“ã¨ãªãã‚ã‹ã£ã¦ããŸæ°—ãŒã™ã‚‹ã€‚

ã¨ã„ã†æ„Ÿã˜ã§ã€Keras ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ç…§ã‚‰ã—åˆã‚ã›ã¤ã¤ã‚³ãƒ¼ãƒ‰ã‚’æ•´ç†ã—ãªãŒã‚‰ã€å…¨éƒ¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã¤ã‘ã¦ã¿ãŸã®ãŒä»¥ä¸‹ã€‚

``` python
# -*- coding: utf-8 -*-

import numpy as np
np.random.seed(20160715) # ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®š

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.utils import np_utils

# MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# å¤‰æ›å‰ï¼š28 x 28 ã®2æ¬¡å…ƒé…åˆ— x 60,000
# å¤‰æ›å¾Œï¼š784è¦ç´ ã®1æ¬¡å…ƒé…åˆ— x 60,000ï¼ˆ256éšèª¿ã‚’ 0 ã€œ 1 ã«æ­£è¦åŒ–ï¼‰
X_train = X_train.reshape(60000, 784).astype('float32') / 255
X_test  = X_test.reshape(10000, 784).astype('float32') / 255

# å¤‰æ›å‰ï¼š0 ã€œ 9 ã®æ•°å­— x 60,000
# å¤‰æ›å¾Œï¼š10è¦ç´ ã®1æ¬¡å…ƒé…åˆ—ï¼ˆone-hot è¡¨ç¾ï¼‰ x 60,000
#         - 0 : [1,0,0,0,0,0,0,0,0,0]
#         - 1 : [0,1,0,0,0,0,0,0,0,0]
#         ...
Y_train = np_utils.to_categorical(y_train, 10)
Y_test  = np_utils.to_categorical(y_test, 10)

# ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ¢ãƒ‡ãƒ«
model = Sequential()

# éš ã‚Œå±¤ 1
# - ãƒãƒ¼ãƒ‰æ•°ï¼š512
# - å…¥åŠ›ï¼š784æ¬¡å…ƒ
# - æ´»æ€§åŒ–é–¢æ•°ï¼šrelu
# - ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ¯”ç‡ï¼š0.2
model.add(Dense(512, input_dim=784))
model.add(Activation('relu'))
model.add(Dropout(0.2))

# éš ã‚Œå±¤ 2
# - ãƒãƒ¼ãƒ‰æ•°ï¼š512
# - æ´»æ€§åŒ–é–¢æ•°ï¼šrelu
# - ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ¯”ç‡ï¼š0.2
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))

# å‡ºåŠ›å±¤
# - ãƒãƒ¼ãƒ‰æ•°ï¼š10
# - æ´»æ€§åŒ–é–¢æ•°ï¼šsoftmax
model.add(Dense(10))
model.add(Activation('softmax'))

# ãƒ¢ãƒ‡ãƒ«ã®è¦ç´„ã‚’å‡ºåŠ›
model.summary()

# å­¦ç¿’éç¨‹ã®è¨­å®š
# - ç›®çš„é–¢æ•°ï¼šcategorical_crossentropy
# - æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼šrmsprop
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# å­¦ç¿’
# - ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼š128
# - å­¦ç¿’ã®ç¹°ã‚Šè¿”ã—å›æ•°ï¼š20
model.fit(X_train, Y_train,
          batch_size=128,
          nb_epoch=20,
          verbose=1,
          validation_data=(X_test, Y_test))

# è©•ä¾¡
score = model.evaluate(X_test, Y_test, verbose=0)
print('Test loss :', score[0])
print('Test accuracy :', score[1])
```

æ´»æ€§åŒ–é–¢æ•°ãƒ»ç›®çš„é–¢æ•°ãƒ»æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ›¸ç±](http://nnadl-ja.github.io/nnadl_site_ja/chap1.html) ã‚’èª­ã‚“ã§ã„ãŸã®ã§ã€ã–ã£ãã‚Šã¨ä½•è€…ãªã®ã‹ã¯ã‚ã‹ã£ã¦ã„ãŸã‘ã©ã€ãã‚Œãã‚Œã©ã†ã„ã†æ„Ÿã˜ã§é¸æŠã™ã‚Œã°ã„ã„ã®ã‹ã¯ã¾ã ã•ã£ã±ã‚Šã‚ã‹ã£ã¦ãªã„ã€‚ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ¯”ç‡ã«ã¤ã„ã¦ã‚‚ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã«è¨­å®šã™ã‚‹ã‚‚ã®ã‚‰ã—ã„ã€ãã‚‰ã„ã®ç†è§£ã€‚ã¨ã„ã†æ„Ÿã˜ã§ã€ç´°ã‹ã„ã¨ã“ã‚ã®ç†è§£ã¯è¿½ã„ã¤ã„ã¦ã„ãªã„ã‘ã‚Œã©ã€é›£ã—ãã†ãªæ•°å¼é”ã‚’ç›´æ„Ÿã§ç†è§£ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã¾ã§ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã—ã¦ãã‚Œã¦ã„ã‚‹ Keras ã¯ã™ã”ã„ã¨æ€ã†ã€‚ã“ã“ã¾ã§ã§ãã‚Œã°ã€å°‘ã—ãšã¤è¨­å®šã‚’å¤‰ãˆã¦ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ãƒ‰ã‚¨ãƒ©ãƒ¼ã§ã‚ã‚‹ç¨‹åº¦é€²ã‚“ã§ã„ã‘ã‚‹æ°—ãŒã™ã‚‹ã€‚

ã¨ã„ã†ã“ã¨ã§ã€æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰ã¯ GitHub ã«ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚

https://github.com/m0t0k1ch1/keras-sample/blob/master/mnist_mlp.py

å†’é ­ã§ã‚‚ç´¹ä»‹ã—ãŸ [ã“ã¡ã‚‰ã®ã‚¨ãƒ³ãƒˆãƒª](http://aidiary.hatenablog.com/entry/20160328/1459174455) ã‚’å‚è€ƒã«ã€å­¦ç¿’éç¨‹ã‚’ã‚°ãƒ©ãƒ•ã§å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‚’ä»˜ã‘åŠ ãˆã¦ã„ã¾ã™ã€‚

å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚°ãƒ©ãƒ•ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ãã¡ã‚“ã¨ loss ãŒæ¸›å°‘ã—ã¦ã‚‹ã€‚

{{< figure src="/img/entry/keras-history.png" >}}

## æ¬¡

[DeepMind](https://deepmind.com) ã®æ€æƒ³ã«è¿‘ã¥ã„ã¦ã„ããŸã„ã®ã§ã€[DQNã‚’Kerasã¨TensorFlowã¨OpenAI Gymã§å®Ÿè£…ã™ã‚‹](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html) ã£ã½ã„ã“ã¨ã‚’ã‚„ã£ã¦ã¿ãŸã„ã€‚
